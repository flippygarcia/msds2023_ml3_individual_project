{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60df9259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:05.084720Z",
     "start_time": "2023-06-11T18:56:05.075373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.\n",
       "js\"></script>\n",
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       "if (code_show){\n",
       "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
       "} else {\n",
       "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
       "}\n",
       "code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" \n",
       "value=\"Click here to toggle on/off the raw code.\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.\n",
    "js\"></script>\n",
    "<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
    "} else {\n",
    "$('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" \n",
    "value=\"Click here to toggle on/off the raw code.\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4906a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:05.124951Z",
     "start_time": "2023-06-11T18:56:05.086708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98ddd8",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Twice</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cd4f3",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/ML3_Twice_Title.png\" width=\"90%\" height=\"90%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb69886",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Introduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5a32d",
   "metadata": {},
   "source": [
    "Are you new to K-pop and find associating the name with a face difficult? Are you already following your favorite K-pop group but still cannot answer the common question, who is your bias? Because you cannot remember the name. With the help of Artificial Intelligence (AI), we will create a face recognition algorithm to help you associate the name with the face so you spend more time being a fan and supporting them rather than just trying to remember which is which.\n",
    "\n",
    "We will use Twice as the K-pop group to establish face recognition for our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685213d",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Background of Twice</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89087849",
   "metadata": {},
   "source": [
    "**One in a Million Annyeong Haseyo Twice Imnida!**\n",
    "\n",
    "Twice is a K-pop girl group comprising nine members: Jihyo, Nayeon, Jeongyeon, Momo, Sana, Mina, Dahyun, Chaeyoung, and Tzuyu. Known for their catchy tunes, vibrant visuals, and energetic performances, Twice has garnered a massive global fanbase. Under the production company JYP Entertainment Co., Ltd, the group finds success in the K-pop industry locally and internationally.\n",
    "\n",
    "The group debuted last October 20, 2015, through a survival show, Sixteen. Now, the group has been performing together for over seven years. They debuted with an extended play (EP) named *The Story Begins* with the lead single *Like Ooh-Ahh*. Since then, the group has released numerous singles and albums that dominated music charts. Their popularity exploded with tracks such as *Cheer Up*, *TT*, *Fancy*, and *What is Love?*. With their fans called *Once*, they supported the group in every release. Now in 2023, Twice are now on its 5th World Tour, still dominating the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841858d",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Motivation</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc4acc",
   "metadata": {},
   "source": [
    "To those new to K-pop, it is difficult to distinguish and memorize the names and associate the name with a face. For me, this task of remembering the name of Twice took months. With the power of Artificial Intelligence (AI), we can create a face recognition for Twice to help new fans identify which member it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15657dc5",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Methodology</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c0af2",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/ML3_Twice_Methodology.png\" width=\"90%\" height=\"90%\"></center>\n",
    "<center>Image 01. Project Methodology.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ec264",
   "metadata": {},
   "source": [
    "**Data Collection**\n",
    "\n",
    "The images of Twice are downloaded from the gallery of its official website.<sup>[4]</sup> It is manually sorted into a folder containing the name of the member (1 folder for each member). Each image used is just a solo picture meaning only the face of the Twice member can be seen. For this project, we used 20 images for each member (180 total solo images).\n",
    "<br><br>\n",
    "\n",
    "**Face Detection**\n",
    "\n",
    "`MTCNN` was used as the pre-trained model for *Face Detection*. This model will detect the faces within the image and provide a bounding box on the face. The image will then be cropped according to this bounding box to retrieve only the image of the face.\n",
    "<br><br>\n",
    "\n",
    "**Face Classification**\n",
    "\n",
    "The cropped-face images of Twice will be split into 60% Training (12 images), 20% Validation (4 images), and 20% Testing (4 images). `Inception-ResNetV1` pre-trained on `VGGFace2` was used to classify cropped-face images into their respective correct name. *Feature Extraction* approach was used to tailor-fit the model towards the dataset. \n",
    "\n",
    "Combining the concept of *Face Detection* and *Face Classification*, we can now determine who is that Twice member? A sample group image was used to illustrate how this would work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39555c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Deep Learning on Images</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822ed32",
   "metadata": {},
   "source": [
    "The deep learning method, as mentioned above, has two parts: *Face Detection* and *Face Classification*. \n",
    "\n",
    "For *Face Detection* I used the model based on the paper by Zhang et al. in 2016 titled [Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks](https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf). Shown below is the architecture of the model.\n",
    "\n",
    "<center><img src=\"./images/MTCNN_architecture.jpg\" width=\"70%\" height=\"70%\"></center>\n",
    "<center>Image 02. MTCNN Model Architecture.<sup>[1]</sup></center>\n",
    "\n",
    "The model has three stages:\n",
    "1. Proposal Network (P-Net): Produces candidate windows and their bounding boxes.\n",
    "2. Refine Network (R-Net): Rejects many false candidates.\n",
    "3. Output Network (O-Net): Describes the face in more detail (outputs five facial landmarks' positions).\n",
    "\n",
    "This pre-trained model no longer needs feature extraction or fine-tuning since I will use it at its intended use, that is, to detect faces (including bounding box and landmark positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88791ba4",
   "metadata": {},
   "source": [
    "For Face Classification, the model I used is based on the paper by Szegedy et al. in 2016 titled [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261.pdf). Shown below is the architecture of the model.\n",
    "\n",
    "<center><img src=\"./images/InceptionResnetV1_architecture.jpg\" width=\"30%\" height=\"50%\"></center>\n",
    "<center>Image 03. Inception-ResNet-v1 Model Architecture. <sup>[2]</sup></center>\n",
    "\n",
    "The complete architecture of Inception-ResNet-v1 is shown in its paper. To summarize, here are what each block does in the model.\n",
    "1. Stem: responsible for extracting basic features of the image (i.e., edge detection and simple patterns).\n",
    "2. Inception-Resnet: consists of multiple parallel convolutional layers with different kernel sizes to capture local and global features. It also has residual connections to address the vanishing gradient problem.\n",
    "3. Reduction: downsamples the spatial dimension of the feature map but increases the number of channels.\n",
    "\n",
    "This *Face Classification* model is pre-trained on VGGFace2, which is based on the paper by Cao et al. in 2016 titled [VGGFace2: A dataset for recognizing faces across pose and age](https://arxiv.org/pdf/1710.08092v2.pdf).<sup>[3]</sup> The dataset contains 3.31 million photos of 9131 subjects. Images are downloads from Google Images, with varying poses, age, illumination, ethnicity, and profession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a03eda",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Setting Up</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d38fcf",
   "metadata": {},
   "source": [
    "Before everything, we must first install the necessary libraries. We used the following non-standard Python libraries\n",
    "\n",
    "* `Pillow`, `scikit-image`, `scikit-learn`, `torch`, `torchvision`, `torchsummary`, `mtcnn`, and `facenet-pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cc5cde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:05.136035Z",
     "start_time": "2023-06-11T18:56:05.131340Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install Pillow\n",
    "# pip install scikit-image\n",
    "# pip install scikit-learn\n",
    "# pip install torch\n",
    "# pip install torchvision\n",
    "# pip install torchsummary\n",
    "# pip install mtcnn\n",
    "# pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c5c13",
   "metadata": {},
   "source": [
    "After installing the non-standard Python libraries, let us import all necessary libraries.\n",
    "\n",
    "**Basic Imports:** `os`, `sys`, `time`, `copy`, `shutil`, `zipfile`, `numpy`, and `matplotlib`.\n",
    "\n",
    "**Image Processing:** `PIL`, and `skimage`.\n",
    "\n",
    "**Deep Learning:** `scikit-learn`, `torch`, `torchvision`, `torchsummary`, `mtcnn`, and `facenet_pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16adb68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:05.146328Z",
     "start_time": "2023-06-11T18:56:05.140434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick fix for permission error\n",
    "import os\n",
    "os.environ['SKIMAGE_DATADIR'] = '/tmp/.skimage_cache'\n",
    "os.environ['XDG_CACHE_HOME'] = '/home/msds2023/fgarcia/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a956a240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:09:38.020391Z",
     "start_time": "2023-06-11T19:09:38.008286Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "import skimage.io as skio\n",
    "\n",
    "# Deep Learning\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,\n",
    "                             accuracy_score, precision_score,\n",
    "                             recall_score, f1_score)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from mtcnn import MTCNN\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fb116",
   "metadata": {},
   "source": [
    "Let us also check if there is an available GPU for Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee99f2a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:11.114461Z",
     "start_time": "2023-06-11T18:56:11.049462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Device is cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Your Device is {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579ee98",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Data Source</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e648d",
   "metadata": {},
   "source": [
    "All Twice images used in this project are downloaded from the official website.<sup>[4]</sup> These images are owned by JYP Entertainment Co., Ltd. The images used are curated from all available Twice images on the website. There are two data folders used in this notebook: `raw_dataset.zip` and `group_image.zip` both of which can be downloaded from this google drive: \n",
    "* `raw_dataset.zip:` https://drive.google.com/file/d/19XYwiqSXk8YPJCRmxjlIhx3i2MR14H58/view?usp=sharing\n",
    "* `group_image.zip:` https://drive.google.com/file/d/108_LuD1eIunF-O5Y4hHsMId7UxOsqqaC/view?usp=sharing\n",
    "\n",
    "To use this dataset, first create a folder named `data` at the same level as your Python notebook. Move the downloaded `.zip` files inside the `data` folder and use the extract code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1458646e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:11.122778Z",
     "start_time": "2023-06-11T18:56:11.118577Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment this code to extract the solo image dataset\n",
    "# with zipfile.ZipFile(f\"./data/raw_dataset.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"./data/raw_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dea502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:11.134555Z",
     "start_time": "2023-06-11T18:56:11.131441Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment this code to extract the group image dataset\n",
    "# with zipfile.ZipFile(f\"./data/group_image.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f80592",
   "metadata": {},
   "source": [
    "Now that we have extracted our dataset and set up all the necessary libraries, let us create our own Twice face recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626f3ae",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Face Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7faeb6",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">MTCNN</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7111760",
   "metadata": {},
   "source": [
    "We initialize the pre-trained model and try to use it first on a sample image to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de477e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:09:22.608891Z",
     "start_time": "2023-06-11T19:09:22.317346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea85378f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:52:26.686683Z",
     "start_time": "2023-06-11T20:52:25.099943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Sample_FaceDetection.png\" alt=\"plots\" style=\"display:block;margin-left:auto;margin-right:auto;width:40%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of MTCNN:\n",
      "{'box': [288, 153, 245, 313], 'confidence': 0.9992341995239258, 'keypoints': {'left_eye': (347, 281), 'right_eye': (460, 265), 'nose': (403, 342), 'mouth_left': (376, 398), 'mouth_right': (463, 386)}}\n"
     ]
    }
   ],
   "source": [
    "# Load sample image\n",
    "img = skio.imread(f'./data/raw_dataset/dahyun/dahyun (10).jpg')\n",
    "\n",
    "# Surpress the loading printout by the mtcnn library\n",
    "stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Run the Pre-trained Model on our image and save the output\n",
    "faces = detector.detect_faces(img)\n",
    "\n",
    "# Restore stdout\n",
    "sys.stdout.close()\n",
    "sys.stdout = stdout\n",
    "\n",
    "# Plot the image\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "# Loop for all the faces detected by the model\n",
    "for face in faces:\n",
    "    # Add the bounding box\n",
    "    x, y, w, h = face['box']\n",
    "    rectangle = Rectangle((x, y), w, h, fill=False, color='red')\n",
    "    ax.add_patch(rectangle)\n",
    "\n",
    "    # Add left eye patch\n",
    "    le_x, le_y = face['keypoints']['left_eye']\n",
    "    le_circ = Circle((le_x, le_y), 5, fill=True, color='red')\n",
    "    ax.add_patch(le_circ)\n",
    "\n",
    "    # Add right eye patch\n",
    "    re_x, re_y = face['keypoints']['right_eye']\n",
    "    re_circ = Circle((re_x, re_y), 5, fill=True, color='red')\n",
    "    ax.add_patch(re_circ)\n",
    "\n",
    "    # Add nose patch\n",
    "    n_x, n_y = face['keypoints']['nose']\n",
    "    n_circ = Circle((n_x, n_y), 5, fill=True, color='red')\n",
    "    ax.add_patch(n_circ)\n",
    "\n",
    "    # Add left mouth patch\n",
    "    lm_x, lm_y = face['keypoints']['mouth_left']\n",
    "    lm_circ = Circle((lm_x, lm_y), 5, fill=True, color='red')\n",
    "    ax.add_patch(lm_circ)\n",
    "\n",
    "    # Add right mouth patch\n",
    "    rm_x, rm_y = face['keypoints']['mouth_right']\n",
    "    rm_circ = Circle((rm_x, rm_y), 5, fill=True, color='red')\n",
    "    ax.add_patch(rm_circ)\n",
    "\n",
    "# Adds context to the image\n",
    "ax.set_title('Face Detection using MTCNN')\n",
    "ax.set_axis_off()\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig(\"./images/Sample_FaceDetection.png\")\n",
    "\n",
    "# Create an HTML img tag to display the image\n",
    "img_tag = (f'<img src=\"./images/Sample_FaceDetection.png\" alt=\"plots\" style='\n",
    "           '\"display:block;margin-left:auto;margin-right:auto;width:40%;\">')\n",
    "\n",
    "# Display the img tag in the Jupyter Notebook\n",
    "display(HTML(img_tag))\n",
    "plt.close()\n",
    "\n",
    "# Display the output of the model . It contains a list of dictionaries\n",
    "# Where each dictionary is a face the keys are:\n",
    "# The bounding box, confidence, and keypoints (landmark positions)\n",
    "print('\\nOutput of MTCNN:')\n",
    "print(faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c2790",
   "metadata": {},
   "source": [
    "`MTCNN` outputs a dictionary containing the coordinates of the bounding box, the confidence of the model on its prediction, and the coordinates of the keypoints of the face (left_eye, right_eye, nose, mouth_left, mouth_right).\n",
    "\n",
    "For our use case, we must create a cropped image based on the face's bounding box. This cropping will create an entire face image (no body or background). These cropped images are preferred for the *Face Classification* model later to have more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51aa45",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Face Extraction</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1064e",
   "metadata": {},
   "source": [
    "We will do the image cropping for all the images and then save them in a directory (`./data/cropped_images/`). The images will be automatically sorted to the correct folder indicating its name since we are using an already sorted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d937cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:14.245753Z",
     "start_time": "2023-06-11T18:56:14.240539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Surpress the loading printout by the mtcnn library\n",
    "stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Create directory for the cropped images\n",
    "crop_dir = './data/cropped_images'\n",
    "if os.path.exists(crop_dir):\n",
    "    shutil.rmtree(crop_dir)\n",
    "os.makedirs(crop_dir)\n",
    "\n",
    "# Loop for all members of twice\n",
    "names = ['chaeyoung', 'dahyun', 'jeongyeon', 'jihyo',\n",
    "         'mina', 'momo', 'nayeon', 'sana', 'tzuyu']\n",
    "for name in names:\n",
    "    # Create directory for individual members\n",
    "    twice_dir = crop_dir+f'/{name}'\n",
    "    if os.path.exists(twice_dir):\n",
    "        shutil.rmtree(twice_dir)\n",
    "    os.makedirs(twice_dir)\n",
    "\n",
    "    # Loops thorugh all 20 raw images for each member\n",
    "    for i in range(1, 21):\n",
    "        # Load current image\n",
    "        img = skio.imread(f'./data/raw_dataset/{name}/{name} ({i}).jpg')\n",
    "\n",
    "        # Run the Pre-trained Model on our image and save the output\n",
    "        faces = detector.detect_faces(img)\n",
    "\n",
    "        # Loop for all the faces detected by the model\n",
    "        for idx, face in enumerate(faces):\n",
    "            # Save the cropped image as defined by the bounding box\n",
    "            x, y, w, h = face['box']\n",
    "            skio.imsave(f\"{twice_dir}/{name}_{i}.jpg\",\n",
    "                        img[y:y+h, x:x+w])\n",
    "\n",
    "# Restore stdout\n",
    "sys.stdout.close()\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b99e0",
   "metadata": {},
   "source": [
    "By the end of this *Face Extraction* you will have a new folder inside your `data` folder called `cropped_images`. The new folder contains nine folders corresponding to the Twice members' names. Each folder contains 20 cropped images of the face of that member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43420760",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Face Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaeb89a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Data Preprocessing</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec0d6c",
   "metadata": {},
   "source": [
    "The first step in preprocessing is to set up the folder such that it splits the dataset into training (60% / 12 images), validation (20% / 4 images), and testing (20% / 4 images). The split data is in 3 separate folders, each containing a folder for each class. \n",
    "\n",
    "Let us also show a sample cropped image for each of the members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a0b1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:14.256597Z",
     "start_time": "2023-06-11T18:56:14.248998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy labeled images to train, validation, and test directories\n",
    "def create_dataset(src, dst, range_, class_):\n",
    "    \"\"\"Copy images of class class_ within range_ from src to dst.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src : str\n",
    "        source directory\n",
    "    dst : str\n",
    "        destination directory\n",
    "    range_ : tuple\n",
    "        tuple of min and max image index to copy\n",
    "    class_ : str\n",
    "        image class \n",
    "    \"\"\"\n",
    "    # If existing, delete dir to reset\n",
    "    if os.path.exists(dst):\n",
    "        shutil.rmtree(dst)\n",
    "\n",
    "    # Create destination directory\n",
    "    os.makedirs(dst)\n",
    "\n",
    "    # Copies the image from source to destination directory\n",
    "    fnames = [f'{class_}_{i}.jpg' for i in range(*range_)]\n",
    "    for fname in fnames:\n",
    "        src_file = os.path.join(src, fname)\n",
    "        dst_file = os.path.join(dst, fname)\n",
    "        shutil.copyfile(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ba0be0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:15.242949Z",
     "start_time": "2023-06-11T18:56:14.259156Z"
    }
   },
   "outputs": [],
   "source": [
    "# looping through create_dataset for each class\n",
    "classes = ['chaeyoung', 'dahyun', 'jeongyeon', 'jihyo',\n",
    "           'mina', 'momo', 'nayeon', 'sana', 'tzuyu']\n",
    "for class_ in classes:\n",
    "    # Source directory\n",
    "    src = 'data/cropped_images'\n",
    "\n",
    "    # Train directory\n",
    "    dst = f'data/train_val_test/train/{class_}'\n",
    "    create_dataset(src+'/'+class_, dst, range_=(9, 21), class_=class_)\n",
    "\n",
    "    # Validation directory\n",
    "    dst = f'data/train_val_test/validation/{class_}'\n",
    "    create_dataset(src+'/'+class_, dst, range_=(1, 5), class_=class_)\n",
    "\n",
    "    # Test directory\n",
    "    dst = f'data/train_val_test/test/{class_}'\n",
    "    create_dataset(src+'/'+class_, dst, range_=(5, 9), class_=class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a66b949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:52:35.463684Z",
     "start_time": "2023-06-11T20:52:34.299882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Sample_Faces.png\" alt=\"plots\" style=\"display:block;margin-left:auto;margin-right:auto;width:100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load sample images for each class in the train set\n",
    "chaeyoung = Image.open('data/train_val_test/train/chaeyoung/chaeyoung_10.jpg')\n",
    "dahyun = Image.open('data/train_val_test/train/dahyun/dahyun_10.jpg')\n",
    "jeongyeon = Image.open('data/train_val_test/train/jeongyeon/jeongyeon_10.jpg')\n",
    "jihyo = Image.open('data/train_val_test/train/jihyo/jihyo_10.jpg')\n",
    "mina = Image.open('data/train_val_test/train/mina/mina_10.jpg')\n",
    "momo = Image.open('data/train_val_test/train/momo/momo_10.jpg')\n",
    "nayeon = Image.open('data/train_val_test/train/nayeon/nayeon_10.jpg')\n",
    "sana = Image.open('data/train_val_test/train/sana/sana_10.jpg')\n",
    "tzuyu = Image.open('data/train_val_test/train/tzuyu/tzuyu_10.jpg')\n",
    "\n",
    "# Plot the images\n",
    "fig, ax = plt.subplots(1, 9, figsize=(30, 10))\n",
    "ax[0].imshow(chaeyoung)\n",
    "ax[0].set_title('Chaeyoung', fontsize=20)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].imshow(dahyun)\n",
    "ax[1].set_title('Dahyun', fontsize=20)\n",
    "ax[1].set_axis_off()\n",
    "ax[2].imshow(jeongyeon)\n",
    "ax[2].set_title('Jeongyeon', fontsize=20)\n",
    "ax[2].set_axis_off()\n",
    "ax[3].imshow(jihyo)\n",
    "ax[3].set_title('Jihyo', fontsize=20)\n",
    "ax[3].set_axis_off()\n",
    "ax[4].imshow(mina)\n",
    "ax[4].set_title('Mina', fontsize=20)\n",
    "ax[4].set_axis_off()\n",
    "ax[5].imshow(momo)\n",
    "ax[5].set_title('Momo', fontsize=20)\n",
    "ax[5].set_axis_off()\n",
    "ax[6].imshow(nayeon)\n",
    "ax[6].set_title('Nayeon', fontsize=20)\n",
    "ax[6].set_axis_off()\n",
    "ax[7].imshow(sana)\n",
    "ax[7].set_title('Sana', fontsize=20)\n",
    "ax[7].set_axis_off()\n",
    "ax[8].imshow(tzuyu)\n",
    "ax[8].set_title('Tzuyu', fontsize=20)\n",
    "ax[8].set_axis_off()\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig(\"./images/Sample_Faces.png\")\n",
    "\n",
    "# Create an HTML img tag to display the image\n",
    "img_tag = (f'<img src=\"./images/Sample_Faces.png\" alt=\"plots\" style='\n",
    "           '\"display:block;margin-left:auto;margin-right:auto;width:100%;\">')\n",
    "\n",
    "# Display the img tag in the Jupyter Notebook\n",
    "display(HTML(img_tag))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5f4c9",
   "metadata": {},
   "source": [
    "We will also need to create dataloaders for our model. This will load the images and feed them to our model later. We will also introduce data augmentation (i.e., Random Horizontal Flip, Random Perspective, and Color Jitter) for a more robust model. The cropped images are also resized into 160 by 160 pixels since the pre-trained model (PTM) we will use is trained on such image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccb6262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:24.006768Z",
     "start_time": "2023-06-11T18:56:16.639633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training set directory\n",
    "train_dir = \"data/train_val_test/train\"\n",
    "\n",
    "# Create data transformation to be use in calcultion of mean & std\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=(160, 160)),  # PTM are trained on 160 by 160 px\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "\n",
    "# Compute for the means and stds (for normalization)\n",
    "imgs = torch.stack([img_t for img_t, _ in train_data], dim=3)\n",
    "means = imgs.view(3, -1).mean(dim=1).numpy()\n",
    "stds = imgs.view(3, -1).std(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e5ec23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:24.048102Z",
     "start_time": "2023-06-11T18:56:24.015962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create data transformation for training, validation, and test sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=(160, 160)),  # PTM are trained on 160 by 160\n",
    "        transforms.RandomHorizontalFlip(p=0.3),  # Data augmentation\n",
    "        transforms.RandomPerspective(p=0.3),  # Data augmentation\n",
    "        transforms.ColorJitter(brightness=0.3),  # Data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)  # Normalize the image\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(size=(160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=(160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Dataset directory\n",
    "data_dir = 'data/train_val_test'\n",
    "\n",
    "# loading image data using ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'validation', 'test']}\n",
    "\n",
    "# Dataloaders\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=36, shuffle=True)\n",
    "               for x in ['train', 'validation', 'test']}\n",
    "\n",
    "# Size of datasets\n",
    "dataset_sizes = {x: len(image_datasets[x])\n",
    "                 for x in ['train', 'validation', 'test']}\n",
    "\n",
    "# Class names\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbac457",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Transfer Learning</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e61dd9",
   "metadata": {},
   "source": [
    "Since our dataset is similar to the dataset used by the pre-trained model (PTM) and is small, we will use the PTM as a feature extractor. We will only replace the last dense layer with a more appropriate layer that outputs 9 values corresponding to our 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9015a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:24.706932Z",
     "start_time": "2023-06-11T18:56:24.051117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the PTM with its pre-trained weights base on facial recognition dataset\n",
    "model = InceptionResnetV1(pretrained='vggface2')\n",
    "\n",
    "# Must be set to True to be use in classification\n",
    "model.classify = True\n",
    "\n",
    "# Freeze the layers in our PTM\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f1a1f",
   "metadata": {},
   "source": [
    "The last layer we created flattens the output of the last convolutional layer then it uses a RelU activation function. A dropout was also added to prevent co-adaptation, and the final layer reduces the dimension to 9, corresponding to our 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9a8d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:26.437445Z",
     "start_time": "2023-06-11T18:56:24.715822Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the last layer named logits to the ff layer with 4 output channels\n",
    "model.logits = nn.Sequential(nn.Linear(512*1*1, 512),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(0.4),\n",
    "                             nn.Linear(512, 9),\n",
    "                             )\n",
    "\n",
    "# Transfer the model to device (GPU) for faster training\n",
    "model = model.to(device)\n",
    "\n",
    "# set the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# only the parameters of the classifier are being optimized\n",
    "optimizer = optim.Adam(model.logits.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ec2a8",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Model Training</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1003e7",
   "metadata": {},
   "source": [
    "Now that we have modified our PTM, we can now proceed with the training. We will use 30 epochs in our training and get the model with the highest validation accuracy as our best model. We will also print the loss and accuracy through the training to track the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb4a2cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T18:56:26.450213Z",
     "start_time": "2023-06-11T18:56:26.439839Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"Function that trains the model using the given loss function,\n",
    "    optimizer and number of epochs.\"\"\"\n",
    "    # Tracks training time\n",
    "    since = time.time()\n",
    "\n",
    "    # Saves the best model here later\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Loop for all epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        # Prints only every 5 epoch\n",
    "        if epoch == 0 or epoch % 5 == 4:\n",
    "            print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            # Tracker for loss\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate loss and accuracy of current epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            # Prints only every 4 epoch\n",
    "            if epoch == 0 or epoch % 5 == 4:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Deep copy the model if it beats the current best model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Print out model summary\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in '\n",
    "          f'{time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad1cbdd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:02:10.355175Z",
     "start_time": "2023-06-11T18:56:26.452289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 2.0667 Acc: 0.2778\n",
      "validation Loss: 1.6989 Acc: 0.6111\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.3555 Acc: 0.9630\n",
      "validation Loss: 0.5202 Acc: 0.8889\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.0943 Acc: 0.9815\n",
      "validation Loss: 0.3433 Acc: 0.8889\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.1046 Acc: 0.9722\n",
      "validation Loss: 0.3625 Acc: 0.8889\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9630\n",
      "validation Loss: 0.3415 Acc: 0.9167\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.9537\n",
      "validation Loss: 0.4700 Acc: 0.8889\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9815\n",
      "validation Loss: 0.4113 Acc: 0.8889\n",
      "\n",
      "Training complete in 5m 44s\n",
      "Best val Acc: 0.916667\n"
     ]
    }
   ],
   "source": [
    "# Train the model by calling the train_model function\n",
    "model = train_model(model.cuda(),\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0aa7e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#FDB3A5\">Model Evaluation</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca68c2",
   "metadata": {},
   "source": [
    "For model evaluation, let us first calculate our dataset's Proportion Chance Criterion (PCC) to have a baseline comparison. Our dataset is balanced and consists of 180 faces, corresponding to the 20 faces of each 9 classes (`chaeyoung`, `dahyun`, `jeongyeon`, `jihyo`, `mina`, `momo`, `nayeon`, `sana`, `tzuyu`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc545db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:02:10.384026Z",
     "start_time": "2023-06-11T19:02:10.364710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PCC = 11.11%\n",
      "1.25*PCC = 13.89%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the image count per class and the total image count\n",
    "chaeyoung_cnt = len(os.listdir('./data/cropped_images/chaeyoung'))\n",
    "dahyun_cnt = len(os.listdir('./data/cropped_images/dahyun'))\n",
    "jeongyeon_cnt = len(os.listdir('./data/cropped_images/jeongyeon'))\n",
    "jihyo_cnt = len(os.listdir('./data/cropped_images/jihyo'))\n",
    "mina_cnt = len(os.listdir('./data/cropped_images/mina'))\n",
    "momo_cnt = len(os.listdir('./data/cropped_images/momo'))\n",
    "nayeon_cnt = len(os.listdir('./data/cropped_images/nayeon'))\n",
    "sana_cnt = len(os.listdir('./data/cropped_images/sana'))\n",
    "tzuyu_cnt = len(os.listdir('./data/cropped_images/tzuyu'))\n",
    "total_cnt = (chaeyoung_cnt + dahyun_cnt + jeongyeon_cnt +\n",
    "             jihyo_cnt + mina_cnt + momo_cnt +\n",
    "             nayeon_cnt + sana_cnt + tzuyu_cnt)\n",
    "\n",
    "# Calculate the PCC\n",
    "pcc = ((chaeyoung_cnt/total_cnt)**2 +\n",
    "       (dahyun_cnt/total_cnt)**2 +\n",
    "       (jeongyeon_cnt/total_cnt)**2 +\n",
    "       (jihyo_cnt/total_cnt)**2 +\n",
    "       (mina_cnt/total_cnt)**2 +\n",
    "       (momo_cnt/total_cnt)**2 +\n",
    "       (nayeon_cnt/total_cnt)**2 +\n",
    "       (sana_cnt/total_cnt)**2 +\n",
    "       (tzuyu_cnt/total_cnt)**2\n",
    "       )\n",
    "print(f\"     PCC = {pcc*100:.2f}%\")\n",
    "print(f\"1.25*PCC = {1.25*pcc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c84ef",
   "metadata": {},
   "source": [
    "The model has a PCC of `11.11%`. This means that 11.11% of the data can be classified correctly due to *chance* alone. With that as our baseline, let us evaluate our model by getting the predictions for our test set. After which, we will calculate the `accuracy`, `precision`, `recall`, and `f1-score`. The confusion matrix was also shown to determine which Twice member our model finds challenging to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4a6c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T19:02:10.400299Z",
     "start_time": "2023-06-11T19:02:10.389013Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Function that test the model using the prepared test loader.\"\"\"\n",
    "    # Initialize counting of correct and total images\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Set no grad since this is evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Load the image and use the model to predict its label\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    return predicted, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9ae20c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:54:15.573150Z",
     "start_time": "2023-06-11T20:54:11.732800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Confusion_Matrix.png\" alt=\"plots\" style=\"display:block;margin-left:auto;margin-right:auto;width:30%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "Accuracy Score: 91.67\n",
      "Precision Score: 92.22\n",
      "Recall Score: 91.67\n",
      "F1 Score: 91.62\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by calling the evaluate function\n",
    "y_pred_tensor, y_true_tensor = evaluate(model, dataloaders['test'])\n",
    "y_pred = y_pred_tensor.cpu().numpy()\n",
    "y_true = y_true_tensor.cpu().numpy()\n",
    "\n",
    "# Display Confusion Matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot(ax=ax, cmap='RdPu')\n",
    "disp.ax_.set_xticklabels(classes, fontsize=8)\n",
    "disp.ax_.set_yticklabels(classes, fontsize=8)\n",
    "disp.im_.colorbar.remove()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.set_title('Model Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./images/Confusion_Matrix.png\")\n",
    "\n",
    "# Create an HTML img tag to display the image\n",
    "img_tag = (f'<img src=\"./images/Confusion_Matrix.png\" alt=\"plots\" style='\n",
    "           '\"display:block;margin-left:auto;margin-right:auto;width:30%;\">')\n",
    "\n",
    "# Display the img tag in the Jupyter Notebook\n",
    "display(HTML(img_tag))\n",
    "plt.close()\n",
    "\n",
    "# Print Evaluation scores\n",
    "print('Model Evaluation:')\n",
    "print(f'Accuracy Score: {accuracy_score(y_true, y_pred)*100:.2f}')\n",
    "print(f'Precision Score: '\n",
    "      f'{precision_score(y_true, y_pred, average=\"macro\")*100:.2f}')\n",
    "print(f'Recall Score: '\n",
    "      f'{recall_score(y_true, y_pred, average=\"macro\")*100:.2f}')\n",
    "print(f'F1 Score: {f1_score(y_true, y_pred,  average=\"macro\")*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94d46d",
   "metadata": {},
   "source": [
    "The model has `91.67%` accuracy, much higher than the calculated PCC. This means that our model is a successful classification since we were able to beat the baseline. We also got a high precision, recall, and F1 score. \n",
    "\n",
    "We could identify which member finds it difficult to classify in the confusion matrix. Here `Tzuyu` is misclassified as `Jeongyeon`, `Mina` is misclassified as `Momo`, and `Momo` is misclassified as `Tzuyu`. These are the members others might find difficult to remember their names and faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bece8a3",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Twice Face Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82692abb",
   "metadata": {},
   "source": [
    "Now that we have a trained model for identifying the members of Twice let us put it to a test. A group image of Twice will be tested, and let us see if the model can correctly associate the faces with the correct names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58051c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:54:47.056905Z",
     "start_time": "2023-06-11T20:54:44.134369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/Twice_Recognition.png\" alt=\"plots\" style=\"display:block;margin-left:auto;margin-right:auto;width:70%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained model\n",
    "detector = MTCNN()\n",
    "\n",
    "# Load sample image\n",
    "img = skio.imread(f\"./data/group_image/twice_0.jpg\")\n",
    "\n",
    "# Surpress the loading printout by the mtcnn library\n",
    "stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Saves the output of the model (including bounding box)\n",
    "faces = detector.detect_faces(img)\n",
    "\n",
    "# Restore stdout\n",
    "sys.stdout.close()\n",
    "sys.stdout = stdout\n",
    "\n",
    "# If existing, delete dir to reset\n",
    "face_recog_dir = 'data/face_recog'\n",
    "if os.path.exists(face_recog_dir):\n",
    "    shutil.rmtree(face_recog_dir)\n",
    "\n",
    "# Create destination directory\n",
    "os.makedirs(f\"{face_recog_dir}/test/dahyun\")\n",
    "\n",
    "# Loop for all the faces detected by the model\n",
    "for idx, face in enumerate(faces):\n",
    "    # Save the cropped image as defined by the bounding box\n",
    "    x, y, w, h = face['box']\n",
    "    skio.imsave(f\"./{face_recog_dir}/test/dahyun/sample_{idx}.jpg\",\n",
    "                img[y:y+h, x:x+w])\n",
    "\n",
    "# loading cropped image data using ImageFolder\n",
    "face_datasets = datasets.ImageFolder(os.path.join(face_recog_dir, 'test'),\n",
    "                                     data_transforms['test'])\n",
    "\n",
    "# Cropped Image Dataloaders\n",
    "face_dataloaders = DataLoader(face_datasets, batch_size=9, shuffle=False)\n",
    "\n",
    "# Get batch of images from the test DataLoader\n",
    "images, _ = next(iter(face_dataloaders))\n",
    "images = images.to(device)\n",
    "\n",
    "# Predicts the images using the trained model\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Gets the class labels from the predictions\n",
    "names = [classes[i] for i in predicted]\n",
    "\n",
    "# Plot the image\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(img)\n",
    "\n",
    "# Loop for all the faces detected by the model\n",
    "for idx, face in enumerate(faces):\n",
    "    # Add the bounding box\n",
    "    x, y, w, h = face['box']\n",
    "    rectangle = Rectangle((x, y), w, h, fill=False, color='#FE7BA3')\n",
    "    ax.add_patch(rectangle)\n",
    "\n",
    "    # Add the face labels\n",
    "    bbox_props = dict(boxstyle=\"round,pad=0.2\", fc=\"#FE7BA3\", ec=\"#FE7BA3\")\n",
    "    ax.text(x+3, y-5, names[idx], fontsize=7, color='white', bbox=bbox_props)\n",
    "\n",
    "# Adds context to the image\n",
    "ax.set_title(f'\\nFace Recognition of Twice')\n",
    "ax.set_axis_off()\n",
    "# plt.show()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig(\"./images/Twice_Recognition.png\")\n",
    "\n",
    "# Create an HTML img tag to display the image\n",
    "img_tag = (f'<img src=\"./images/Twice_Recognition.png\" alt=\"plots\" style='\n",
    "           '\"display:block;margin-left:auto;margin-right:auto;width:70%;\">')\n",
    "\n",
    "# Display the img tag in the Jupyter Notebook\n",
    "display(HTML(img_tag))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23a9ff",
   "metadata": {},
   "source": [
    "Even at `91.67%` accuracy, we could correctly predict the sample group image. To further boost our model's performance, adding more images would be beneficial. But at its current state, our model is enough to help the new Once in recognizing each Twice member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32510eb",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">Key Takeaways</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c97a68",
   "metadata": {},
   "source": [
    "Deep Learning methods can be applied even in the use-case of being a K-pop fan. We demonstrated in this project that:\n",
    "* There are available pre-trained model to do *Face Detection* and *Face Classification*\n",
    "* Even with a small dataset (12 training images and 4 validation images), we can tune (by Feature Extraction) the model toward our specific purpose\n",
    "* Analyzing the confusion matrix, we could determine which member Twice, people might find it difficult to associate the name with the face\n",
    "\n",
    "Now you know how to do Face Recognition using the Deep Learning method. You can simply swap out the dataset to another K-pop group you like and make a Face Recognition model yourself. \n",
    "\n",
    "A great question to ask now is, Are you already a Once? And also, did you catch who is my bias in Twice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cee2d",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#FE7BA3; color:#F5F5F1; padding: 10px;\">References</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989b63c",
   "metadata": {},
   "source": [
    "[1] K. Zhang, Z. Zhang, Z. Li and Y. Qiao. (2016). *Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks*, IEEE Signal Processing Letters.\n",
    "\n",
    "[2] Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. (2016). *Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning*. In Proceedings of the Computer Vision and Pattern Recognition (CVPR) (pp. 4278-4284).\n",
    "\n",
    "[3] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman (2018). *VGGFace2: A dataset for recognising faces across pose and age*. International Conference on Automatic Face and Gesture Recognition.\n",
    "\n",
    "[4] JYP Entertainment Co., Ltd (n.d.) *Twice: Gallery*. Retrieved June 11, 2023 from https://twice.jype.com/Default/Gallery\n",
    "\n",
    "[5] OpenAI. (2023). *ChatGPT* (Mar 14 version) [Large language model]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
